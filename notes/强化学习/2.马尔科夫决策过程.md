# Markov决策过程

## 基本框架
1. 问题：
    1. state:状态是当前环境的一种形态，比如对于围棋来说，每一种棋子与棋盘的组合都是一种状态，关键是我们能够直到自己处于什么状态；
    2. model/转换函数:模型是环境运行的基础，基于概率，输入状态s，该状态下做的动作a，以及期望转换到的状态s'，得到一个转换概率；
    3. action:在环境下所有能做的动作集；
    4. reward:三种形式 R(s):进入某种状态给予的奖励，R(s,a)进入某种状态后进行动作a的奖励，R(s,a,s')进入某状态后进行动作a从而进入状态s'的奖励，数学上这三种形式是相等的；
2. 答案：
    1. policy:策略指的是一种映射，Z(s)->a，也就是每种状态下需要做出的最大奖励的动作，策略只关注当下，不会考虑以前和未来，所以貌似容易陷入局部优化问题吧；

## 奖励
信用分配问题


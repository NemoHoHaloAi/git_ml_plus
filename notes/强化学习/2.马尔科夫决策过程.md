# Markov决策过程

## 基本框架
1. 问题：
    1. state:状态是当前环境的一种形态，比如对于围棋来说，每一种棋子与棋盘的组合都是一种状态，关键是我们能够知道自己处于什么状态；
    2. model/转换函数:模型是环境运行的基础，基于概率，输入状态s，该状态下做的动作a，以及期望转换到的状态s'，得到一个转换概率；
    3. action:在环境下所有能做的动作集；
    4. reward:三种形式 R(s):进入某种状态给予的奖励，R(s,a)进入某种状态后进行动作a的奖励，R(s,a,s')进入某状态后进行动作a从而进入状态s'的奖励，数学上这三种形式是相等的；
2. 答案：
    1. policy:策略指的是一种映射，Z(s)->a，也就是每种状态下需要做出的最大奖励的动作，策略只关注当下，不会考虑以前和未来，所以容易陷入局部最优问题；

## 奖励
信用分配问题

**关于奖励**：假设每一步都有一个奖励值a，目的地状态有一个奖励值b，错误地状态有一个奖励值c，已知目的地和错误地都是结束状态，那么当我们在设计a,b,c的时候，
对于他们的大小要特别注意，例如我们训练狗狗把扔出去的球接回来，狗狗每做一个动作，奖励为a，如果它成功把球接回来，奖励b，如果它不仅没接回球，还咬人，那么奖励为c，
首先b肯定为正数，比如给它一个大零食，c肯定为负数，比如打他一下，关键是a的取值，假设a为正数，意思就是说不管有没有取回球，只要狗狗做出一个动作，我们就给出一个正面的奖励，
比如给它一个小零食，这会导致狗狗不积极的去取球，因为它一直得到小零食，那么它只要把动作做很多，就能得到很多小零食，所以a不能为正，那么a为负数呢？假设a为-2，比如打他一顿，
那么此时由于惩罚太重，甚至超过了它进行错误的状态，那么它宁愿进入错误的状态，被你打一下，也不愿意因为动作被打一顿，假设a为-0.1呢，比如吓唬它一下，此时的情况是：
完成目的-大零食，错误地状态-打一下，每一步-吼一下，虽然每一步都是惩罚，但是这个惩罚比进入错误地要小很多，因此它会尽可能快的进入目的地状态来获取零食；

## 超大单步奖励、超大单步惩罚的例子
每一步有0.8的可能成功，有0.2的可能进入直角状态，T表示目的地，E表示错误地，N表示不能进入，x为求该位置下要做出的动作：

1. 目的地奖励+1，错误地奖励-1，每一步+2：
    
        |   |   | x1| T |
        |   | N | x2| E |
        |   |   | x3| x4|

    由于每一步奖励太积极了，因此我们倾向于一直留在迷宫中积累奖励，
    x1：向上、向下都有0.1的可能进行T导致结束，向右同理，因此应该是向左；
    x2：同x1，向上、向下都有0.1的可能进入E，向右也是，因此应该是向左，由于左边是墙壁，那么它就会停在原地；
    x3：上下左右都没有结束状态，因此在此种状态下，如何行动都可以；
    x4：向左、向右、向上都有可能进入E，因此应该是向下；

2. 目的地奖励+1，错误地奖励-1，每一步-2：
    
        |   |   | x1| T |
        |   | N | x2| E |
        |   |   | x3| x4|

    由于每一步奖励太消极了，因此我们倾向于赶紧结束这一切，
    x1：向上、向下、向左都会得到-2的奖励，那么应该是向右；
    x2：向上、向下、向左都会得到-2的奖励，那么应该是向右；
    x3：各个方向都会先得到-2的奖励，那么就要选择一个能尽快结束的方向，应该是向右；
    x4：向左、向右、向下都会得到-2的奖励，那么应该是向上；

3. 结论：能看出奖励值几乎决定了算法的整个走向；

## 奖励序列
* 时间序列：之前我们都是假设无穷时域的前提下给出的策略；
* 效用序列：**偏向稳定性**，假如序列U1(s,s1,s2....)>序列U2(s,s'1,s'2....)，那么总是可以得到结论U(s1,s2....)>U(s'1,s'2....)；

数学上分析偏向稳定性：

    U(s0,s1,s2....) -> sum(R(s_t))

    通常为了避免无穷时域导致的序列问题，一般会加入另一个变量到公式中：

    U(s0,s1,s2....) -> sum(beta^t*R(s_t)) 其中：0<=beta<1
    也就是说随着时间t的变大，单位奖励会越来越小；

## 策略
bulabulabulabula......

## 效用与奖励
奖励关注的是一个个不同状态的奖励，而效用更像是一个长期的奖励；

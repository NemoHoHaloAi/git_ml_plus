# 强化学习简介

## 重点关注
1. 强化学习如何进入原始工作流程的模型构建阶段；
2. 评估由强化学习产生的不同模型的优势和不足；
3. 强化学习与目前学习的其他机器学习算法产生的模型的不同；

## 理解
若将状态看作为属性，动作看作为标记，易知：监督学习和强化学习都是在试图寻找一个映射，从已知属性/状态推断出标记/动作，这样强化学习中的策略相当于监督学习中的分类/回归器。但在实际问题中，强化学习并没有监督学习那样的标记信息，通常都是在尝试动作后才能获得结果，因此强化学习是通过反馈的结果信息不断调整之前的策略，从而算法能够学习到：在什么样的状态下选择什么样的动作可以获得最好的结果。

## 基本要素
强化学习任务通常使用马尔可夫决策过程（Markov Decision Process，简称MDP）来描述，具体而言：机器处在一个环境中，每个状态为机器对当前环境的感知；机器只能通过动作来影响环境，当机器执行一个动作后，会使得环境按某种概率转移到另一个状态；同时，环境会根据潜在的奖赏函数反馈给机器一个奖赏。综合而言，强化学习主要包含四个要素：状态、动作、转移概率以及奖赏函数。

    状态（X）：机器对环境的感知，所有可能的状态称为状态空间；
    动作（A）：机器所采取的动作，所有能采取的动作构成动作空间；
    转移概率（P）：当执行某个动作后，当前状态会以某种概率转移到另一个状态；
    奖赏函数（R）：在状态转移的同时，环境给反馈给机器一个奖赏。

因此，强化学习的主要任务就是通过在环境中不断地尝试，根据尝试获得的反馈信息调整策略，最终生成一个较好的策略π，机器根据这个策略便能知道在什么状态下应该执行什么动作。

## 对比监督、非监督学习
* 与监督学习的区别：
  * 监督学习：输入到输出的映射，学习到输入输出之间的关系，可以告诉算法什么样的输入对应着什么样的输出；
  * 强化学习：输入到输出的映射，给机器的反馈 reward function，即用来判断这个行为是好是坏；
* 与非监督学习的区别：
  * 非监督学习：并不是学习一个从输入到输出的映射，而是学习数据的模式，比如聚类算法，数据没有严格意义的输出，算法是在学习数据之间的一种隐藏的模式；
  * 强化学习：输入到输出的映射；
* 举例新闻推送：
  * 监督学习：假设新闻有标签，那么就推送给用户跟之前看过的新闻一致的标签的新闻；
  * 非监督学习：通过用户之前看过的新闻，进行聚类，推送属于该聚类的其他没看过的新闻；
  * 强化学习：通过给用户少量的推送，通过用户是否点击、是否浏览、浏览时长等对自身进行奖励或惩罚来调整自己的推送策略；
  

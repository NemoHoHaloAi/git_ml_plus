# 深度神经网络

## 做什么
1. 构建深度神经网络，使用Tensorflow；
2. 识别图像；

## Tensorflow实现ReLU的隐含层
复习：ReLU函数是一个非线性激活函数，公式为max(input,0)

        # Hidden Layer with ReLU activation function
        # 隐藏层用 ReLU 作为激活函数
        hidden_layer = tf.add(tf.matmul(features, hidden_weights), hidden_biases)
        hidden_layer = tf.nn.relu(hidden_layer)

output = tf.add(tf.matmul(hidden_layer, output_weights), output_biases)

上面的代码把tf.nn.relu() 放到隐藏层，就像开关一样把负权重关掉了。在激活函数之后，添加像输出层这样额外的层，就把模型变成了非线性函数。这个非线性的特征使得网络可以解决更复杂的问题。

## 深度vs宽度
通常来讲对于宽度的增加对模型的性能改善并不大，但是对深度的增加，也就是隐含层的层数的增加，会有效的改善模型的效果，在图像识别可视化中，
我们会发现，刚开始的层学习到的是一些具体的东西，比如线条等，而随着越来越深，模型会学到一些轮廓、部分图像等非常抽象的内容，而这就是我们
想要的，也就是说随着网络变得越来越深，实际上模型拥有了越来越大的特征表达能力，或者说越来越像人；

## 保存加载模型
Tensorflow同样支持对模型的保存和加载，通过tf.train.Saver()对象

## 正则化
针对数据选择模型时，通常我们会选择一个更加泛化的模型，而不是看起来更适合数据的模型，因为更适合的模型通常难以被优化，且存在过拟合问题，
而有足够泛化能力的模型则不会遇到这些问题；

## 防止过拟合的办法
1. 观察验证集数据上的性能指标，当它停止上升时，停止训练；
2. L2正则化；
3. Dropout；

I have no idea;

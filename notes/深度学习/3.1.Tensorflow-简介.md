# Tensorflow

## 深度学习
解决感知问题，例如图像识别、语音识别等，这一类问题的特点是需要算法具有强大的表征能力，因为数据都是基础像素级别的；

## 近代的崛起
1. 海量的数据，得益于大量的传感器等，海量的数据被生成、传播、存储；
2. 便宜、高速的GPU模块；

## 修正线性单元
ReLU，公式为f(x)=max(x,0)，即输出为输入与0中取大的那个，比如输入为-1，那么输出为0，如果输入为0.5，那么输出为0.5，

sigmoid函数的缺陷在于，它的导数最大为0.25，那么在反向传播中，每层出现的错误至少减少75%，如果有很多层，那么权重的
更新将会很小，也就需要更多的训练时间，这是它不适合作为隐含层的激活函数的原因；

## Softmax
Softmax应用于有多个输出时，此时Softmax给出的是每个输出为真的概率，且总和为1；

## Tensorflow入门基础

### 基本名词解析
1. graph-图：使用**图**来表示计算任务；
2. session-会话：在被称之为**会话**的上下文中执行**图**；
3. tensor：使用tensor表示**数据**；
4. variable-变量：使用变量来**维护状态**；
5. feed/fetch：使用feed或fetch为任意的操作赋值或者从中获取数据；

### 综述
TensorFlow 是一个**编程系统**, 使用**图**来表示**计算任务**. 图中的**节点**被称之为 **op** (operation 的缩写). 一个 op **获得** 0 个或多个 Tensor, 执行**计算**, **产生** 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组. 例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 [batch, height, width, channels].

一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是 tensorflow::Tensor 实例.

### 计算图
TensorFlow 程序通常被组织成一个**构建阶段**和一个**执行阶段**. 在构建阶段, op 的执行步骤 被**描述**成一个图（或者说一个流更形象吧）. 在执行阶段, 使用会话**执行**图中的 op.

例如, 通常在构建阶段创建一个图来表示和训练神经网络, 然后在执行阶段反复执行图中的训练 op.

#### 构建阶段-构建图
构建图的第一步, 是创建**源 op** (source op). 源 op **不需要**任何输入, 例如 **常量** (Constant). 源 op 的输出被传递给其它 op 做运算.

下面是一个构建阶段的简单例子，三个节点，其中有两个Constant源op，一个matmul的op：

		matrix1 = tf.constant([[3., 3.]])
	
	
		matrix2 = tf.constant([[2.],[2.]])
	
	
	
		product = tf.matmul(matrix1, matrix2)

为了真正进行矩阵相乘运算, 并得到矩阵乘法的 结果, 你必须在**会话**里**启动**这个图. 

#### 执行阶段-在会话中启动图
构建阶段之后就是执行阶段，首先需要创建一个会话，用于执行图：

		with tf.Session() as sess:
		  result = sess.run([product])
			  print result

一般用with语法块来自动完成会话的关闭操作；

PS：一般不需要指定使用CPU还是GPU，Tensorflow会自动检测，如果检测到GPU，会尽可能的使用第一个找到的GPU，其余GPU
默认是空闲的，如果没有GPU，那么会使用机器的CPU，如果要实现多个GPU，那么可以通过手动指定的方式为每个op指派GPU；

### 交互式使用
为了便于使用诸如 IPython 之类的 Python 交互环境, 可以使用 **InteractiveSession** 代替 Session 类, 使用 **Tensor.eval()** 和 **Operation.run()** 方法代替 Session.run(). 这样可以避免使用一个变量来持有会话.

### Tensor-数据
Tensorflow使用Tensor来表示所有数据，各个op间传入传出的都是Tensor，Tensor可以看做是一个n维的数组列表，在python中用numpy的ndarray表示；

### Variable-变量
变量维护图执行过程中的状态信息. 下面的例子演示了如何使用变量实现一个简单的计数器：

		state = tf.Variable(0, name="counter")
	
		one = tf.constant(1)
		new_value = tf.add(state, one)
		update = tf.assign(state, new_value)
	
	
	
		init_op = tf.initialize_all_variables()
	
	
		with tf.Session() as sess:
		  
		  sess.run(init_op)
		  
		  print sess.run(state)
		  
		  for _ in range(3):
			      sess.run(update)
				      print sess.run(state)

通常变量用来保存训练中的一些参数，在训练过程中被不断更新，同时变量是可以被持久到磁盘中的，方便下次训练、测试时直接加载使用；

### Fetch
如果想要获取多个op的输出结果，那么可以通过向run中增加一个列表op，来获取到结果的列表：

			input1 = tf.constant(3.0)
			input2 = tf.constant(2.0)
			input3 = tf.constant(5.0)
			intermed = tf.add(input2, input3)
			mul = tf.mul(input1, intermed)
		
			with tf.Session():
				  result = sess.run([mul, intermed])
				    print result

以上，result中获得了mul以及add的两个op输出结果，注意是在一次run中获取的，而不是分别多次run；

### Feed
feed 机制可以**临时**替代图中的任意操作中的 tensor 可以对图中任何操作提交补丁, 直接插入一个 tensor.

feed 使用一个 tensor 值临时替换一个操作的输出结果. 你可以提供 feed 数据作为 run() 调用的参数. feed 只在调用它的**方法内有效**, 方法结束, feed 就会消失. 最常见的用例是将某些特殊的操作指定为 "feed" 操作, 标记的方法是使用 **tf.placeholder()** 为这些操作创建占位符.

			input1 = tf.placeholder(tf.types.float32)
			input2 = tf.placeholder(tf.types.float32)
			output = tf.mul(input1, input2)
		
			with tf.Session() as sess:
			  print sess.run([output], feed_dict={input1:[7.], input2:[2.]})





# 神经网络入门
神经网络通过接收一系列的输入，构建隐含层，最终得到一系列输出的结构，因为与人神经元的工作方式有相似点，
因此称之为神经网络，通过对隐含层的神经元个数的增加以及隐含层层数的增加，神经网络可以具备非常强大的
数据表征能力；

## 感知器
https://classroom.udacity.com/nanodegrees/nd009-cn-advanced/parts/6fdfb816-b444-4f2e-b6b3-31cb2aa0d99f/modules/9e76733e-b71e-4fb0-b888-7611a6d35732/lessons/dc37fa92-75fd-4d41-b23e-9659dde80866/concepts/5ab911d0-fe20-4113-852c-8a07fe9bdacc

1. 对于神经网路来说，通常初始状态时并不知道哪个信息更重要，这需要它训练后得到，也就是**权重**；
2. 权重刚开始是随机值，当神经网络学习到什么样的输入数据会使得学生被学校录取之后，网络会根据之前权重下分类的错误来调整权重，这个过程被称为神经网络的训练；
3. 感知器把权重应用于输入再加总的过程叫做**线性组合**，例如:x1w1+x2w2+...xmwm；
4. 感知器求和的结果会被转换成输出信号，这是通过把线性组合传给**激活函数**来实现的，常用的有sigmoid、单位越阶函数等，将线性组合结果转换成输出信号；
5. 函数返回更多 11 的一种方式是往我们线性组合的结果里加上一个**偏置项**（bias），篇置项类似权重，也是随机分配后由算法学习得来的；

## 梯度下降-逼近最优解，在神经网络中一般用于参数更新时
寻找W，使得误差平方和（.4\*sum((yi_pred-yi_real)^2)）最小；

权重的更新公式：学习率\*xi\*error_term，其中学习率为参数，xi为对应的输入值，error_term为(y_read-y_pred)\*激活函数的导数

权重的更新通常会根据实际情况进行一定的放大和缩小，避免出现步长过大，无法获得最小值的情况；

## 多层感知机
涉及矩阵和向量的部分，输入由一个一维向量表现，权重由一个矩阵表示，wij表示输入i到隐含j的权重；

## 反向传播
要使用梯度下降法更新隐藏层的权重，你需要知道各隐藏层节点的误差对最终输出的影响。每层的输出是由两层间的权重决定的，两层之间产生的误差，
按权重缩放后在网络中向前传播。既然我们知道输出误差，便可以用权重来反向传播到隐藏层。

## 到此
到此为止依然属于监督学习的范畴，根据带有标签的样本训练模型参数并进行预测；

# 基于实例的学习

## 原来的算法基本流程
1. 获取到原始数据，例如(x,y)...(x,y)
2. 根据数据学习到一个函数，例如f(x)
3. 然后就遗忘原始数据（此时跟原始数据没关系了）对未知x通过f(x)计算y

## 基于实例的流程
1. 记忆所有原始数据并持久化
2. 当需要计算x时，直接通过db查找对应的y即可

## 对比原来的算法
优势：
1. 有记忆功能，不会抛弃原始数据
2. 速度快，没有模型训练过程，只有db的io操作
3. 简单

劣势：
1. 无法泛化
2. 对噪声敏感，也就是过拟合严重
3. 对于同一个x对应多个y的原始数据，无法给出一个答案

根本：记忆功能无法有效的进行泛化推广，以及解决过拟合问题；

## 简单的解决方案 -- KNN
对于要计算的x，不是简单的取查询它在原始数据中对应的y（避免过拟合问题），而是计算x与原始数据中
所有数据之间的距离（此处就是特征x的绝对差值），选出距离最近的k个数据，计算这k个数据中同一分组
最多的组，那么这个组就是我们对x的预测，一个问题：但是这个如何推广到回归上呢？？

## 回归、分类
分类问题：此时的y是一个类别数据，那么我们就可以取k中y相同最多的那个y作为预测值，这样做会遇到一些
类似出现两个y都有同等数量的问题，此时就需要一些策略去解决；

回归问题：此时的y是一个连续值，那么我们可以将k个y进行加权平均，权重可以由与未知x的距离的倒数来表示；

## 三种算法训练、查询的所需时间、空间
'''

                    时间                空间
        1-NN
            训练    1                   n
            查询    log n               1
        k-NN
            训练    1                   n
            查询    log n + k           1
        线性回归
            训练    n(不解)             1(也就是存储函数的常量参数，比如y=wx+b中的w和b)
            查询    1(就是带入公式计算) 1
'''


结论：能够看到，kNN和线性回归的主要区别在于二者主要耗时的部分，对于kNN它的耗时部分处于查询，而对于
线性回归，它的耗时在于训练，因此kNN被称为lazy，而线性回归被称之为eager，这个特点也决定了二者的使用
场景的不同；

## kNN的问题
某些情况下会出现根据kNN使用的k值不同，以及距离计算方式的不同，得到的结果大相径庭，且并没有哪个结果可以
接近正确值，这种情况一般是实际数据违背了kNN的偏差导致的；

## kNN偏好偏差
偏好偏差，可以理解为倾向：
1. 对于决策树，我们的倾向是越简单越好，越矮越好；
2. 对于kNN我们的倾向就是我们使用的计算距离的公式；

由于之前计算中使用的是欧氏距离和曼哈顿距离，这两个算法都是跟所有输入特征相关的，且假定每个特征的影响
是一样大的，因此出现了后续的偏差很大的问题，因为实际上特征1的影响要远远大过特征2，而这是这两个算法
没有考虑的地方；

## kNN缺陷
关键在于明确距离计算公式，但是由于这个问题需要很深的领域知识，且即便明确了领域知识也不一定能够很好的
预测复杂情况下的特征数据集，因此较适合简单的数据集，且基本明确数据集中的特征的影响基本一致的情况下，
否则还是那些eager算法更好，因为大部分eager算法在训练阶段，训练的就是所谓的特征的影响程度，也就是权重，
比如最典型的神经网络；

## 维度灾难
意指随着特征或者维度的增加，我们需要的能够使得模型具有泛化能力的数据量是呈指数上升的；

理解：
1. 假设现在是一维特征，也就是一条直线或者说线段，描述它需要N个数据；
2. 当特征增加1个也就是二维，也就是xy坐标系，描述它需要N\*N个数据；
3. 同理当特征增加2个也就是三维，也就是三维空间，描述它需要N\*N\*N个数据；

举例：维度灾难在kNN上的体现：
1. 假设一维数据，那么寻求最短距离只需要N个能平均分割直线的即可；
2. 变为二维数据，那么原来的N个平均分割x轴的点肯定是不够了，因为有y轴要考虑，因为需要N\*N个；
3. 变为三维空间，此时又多个一个z轴要考虑，因此需求的数据个数变为N\*N\*N个；

能看到对数据的需求，虽然维度的增加是呈指数上升的，如果原本只需要10个数据，只需要将特征个数增加到4个，
该数据量将暴增至10000个，非常容易失控；

## kNN重点
1. 距离计算方式：选择好的距离计算方式很重要，同时不要简单的认为它就是一个公式，它本质上是一个黑盒子，里面
可以做任何你希望的计算方式、技巧，基于领域知识（例如人脸识别中可能会比对眼睛、嘴巴的区别，而不是整张图片
的差别）；
2. k值的选择：通常来讲是经验得到的，但是此处有一个需要注意的点，就是当获取到k个数据点后如何进行回归或分类；

## 获取到k个数据点后如何进行回归和分类
举例回归：之前用到的平均值是最简单的回归手段，本质上平均值就是一种特殊的回归方式，加权平均是更复杂、考虑的
因素更多了一点的回归方式，而想象空间更大的是此处结合其他机器学习算法来使用，将kNN看做一个选择局部最优数据
的算法，然后通过这些选中的与待预测数据某种程度上最接近的数据集上进行其他机器学习算法的模型训练，再进行预测，
利于先使用kNN选择k个点，再进行线性回归，这样一来可操作性就太大了，kNN算法也就活了；

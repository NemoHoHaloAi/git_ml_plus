# 集成学习
集成学习基本流程：
1. 通过第一部分数据子集学习到某一条规则；
2. 通过第二部分数据子集学习到另一条规则；
3. 同上，直到数据被使用完为止；
4. 组合获得的所有简单规则，合并为一条复杂规则，并达到期望的效果；

使用数据子集获取规则而不是全部数据的原因在于，当面对较少数据时，反而更加容易得到简单的规则，相当于
要描述的数据集更小，那么也就不需要那么多特征；

两个核心关键：
1. 选取子集，首先遵循平均原则，其次子集的大小很重要，这个大小直接决定了通过该数据子集能获取到什么样的简单规则；
2. 怎么合并简单规则，当然了，取平均值永远是最简单的方法；

## bagging
1. 随机平均选择数据子集，并学习规则；
2. 使用平均值方式合并规则；

优势：跟使用全部训练集相比，这种方式既没有浪费数据（本质上也使用了全部数据），同时由于是多个学习结果的
平均值，因此最大限度的消除了过拟合带来的方差、偏差问题，有点类似交叉验证的思路，研究表明，与使用全部
数据学习相比，集成学习通常在训练集上表现会差一些，但是在测试集上表现会更好；

## boosting
看看人家总结的，浅显易懂：
> https://www.jianshu.com/p/708dff71df3a
> http://lib.csdn.net/article/machinelearning/35135

与bagging的不同：
1. 数据子集：不再是平均的随机选择，而是选择“难”的那些样本，也就是之前没有很好分类的样本，深入学习它的规则；
2. 合并方式：还是平均值，但这次是加权平均值；

名词：
1. error：误差新定义，在已知的数据分布中，x的假设不等于x的预测；

        假设已知数据分布有四个点a,b,c,d，当预测与实际中a,b是一致的，c,d是不一致时，误差率为多少？

        由于没有其他信息，因此就是对一半错一半，0.5；

        那么如果直到各个点的出现频率不一致呢，例如a为1/2，b为1/20，c为4/10，d为1/20？

        新的error定义就是针对这个情况，它认为每个数据点对于误差率的影响不应该是一样的，要根据数据点的频率来决定，也就是说
        出现频率更高的点有更大的影响，因此有：(1/20+1/20)/(1) = 0.1

        实际意思就是说：对于那些出现频率很低的数据点，即使没有预测正确，对误差率也没有多大的影响，相反，对于出现频率很高的
        点，如果预测错误，那么会极大的升高误差率；

2. 弱学习：指的是不管什么数据分布，总能得到比1/2小的误差率，也就说总能学习到点什么来帮助自己预测，同时也不会比1/2低多少；

伪代码：
        
        一组训练数据{(Xi,Yi)}
        Y属于{-1,+1}，因此这是个分类问题
        for t = 1 to T:构建时间步
            构建数据在t上的分布
            查找弱分类器
            得到一个误差error
        输出最终分类器
        
        上述问题解决：
            循环问题：
            最终假设问题：

# 决策树

## 分类与回归的区别
* 分类：根据某种输入，预测一个有限的离散型的输出；
* 回归：根据某种输入，预测一个连续的数值；

## 分类学习
* instances：代表一个输入，可以是一张照片、一个人的收入等等；
* concept：理解为是输入到输出的映射关系的函数，比如对于是否是狗，给出狗的各种概念，这些概念就组成了一个从一个输入动物到是否为狗的映射函数；
* target concept：concept中完美表示这种映射关系的那一个函数；
* hypothesis：所有可能的映射关系；
* sample：也就是训练集；
* condidate：候选者，也就是concept中，你认为是target concept的那个，但是还需要验证；
* testing：测试集，数据结构同训练集，但内容不一致，检验模型的泛化能力；

## 决策树的深度
对于一棵决策树来说，永远都是从根节点开始运作，从上到下，其实就是一个信息增益由大到小的过程，信息增益越大的特征，会出现在越上方的位置，这表示该
特征与输出的关联性很强，而越往下，表示该特征能够提供的信息越少，因此限制树的深度，就是在一定程度上避免信息增益过少的特征加入到树中来；

## 机器学习构建决策树模型
实质上就是决定那些特征位于树的什么位置的节点的过程，这一构建过程主要就是通过信息增益作为标准来生成决策树的过程；

## 决策树可能性
对于3个特征的决策树（每个特征取值都是true和false，输出也是true和false），所有组合个数为2^3，也就是8种，如果加上输出，那么就是2^(2^3)，也就是256种，
可以看到对于整个树来说，特征的增加会导致可表达性的可能性以指数形式增多，也就是说对于较多的特征，会对应非常多的组合；

## 决策树中的连续属性
range：通常使用范围来表示，比如对于age，将其拆解为多个特征：小于10、大于10小于20....，也就是在其他维度上描述这个特征，好处有两个：1.可以将连续型转
换为bool型，2.使得该字段有更深层次的含义，比如泰坦尼克幸存者数据中，对于年龄的划分可以更容易体现age字段的作用；

## 是否可以在决策树的特定路径上重复某个属性
* 离散型：没有必要，无意义；
* 连续型：有可能是有意义的，例如之前节点使用过age，条件是是否大于20岁，那么之后的节点依然可以使用age，只要是不同的条件，例如是否大于50岁，那么这种重复就是有意义的，进一步利用了信息；

# 决策树

## 多远线性问题
无法使用一条直线分割的问题，可以通过多个条件的判断来实现分类；

## 构建决策树
1. 第一次分割：尽可能的将数据以垂直于x轴的线分开；
2. 第二次分割：对于左侧数据，再次进行分割；
3. 第三次分割：对于右侧数据，再次进行分割；
4. 重复步骤1,2,3直到基本数据聚集区都被表示为决策树上的分支为止；

记住，最好的分割是尽可能的将两组数据分开，对于决策树，数据的分布越没有规律，会导致决策树分支越多越深；

## 决策树编码
sklearn.tree.DecisionTreeClassifier()，默认的决策树训练预测时存在过拟合现象会影响准确率，这时，可以通过调节参数来控制；

## 决策树参数
min_samples_split：默认为2，代表一个节点至少有多少个数据才能继续向下分裂子节点，这个参数通常不会使用默认的，设置该参数为一个更大的值虽然会导致在训练集上表现差一些，
但是会使得模型更加泛化，因此在测试集上的表现会更好；

## 熵
熵与数据单一性呈负相关的关系，极端情况下，数据都是同一类，那么此时熵为0，如果N个数据均匀的分布在所有类别中，那么此时熵为1.0；

因此决策树分割的过程其实就是在不断的寻找数据单一性的过程的递归实现；

## 熵的计算
例如有如下数据：
grade(倾斜)     bumpiness(颠簸)     speed limit(限速)       speed(速度)
steep           bumpy               yes                     slow
steep           smooth              yes                     slow
flat            bumpy               no                      fast
steep           smooth              no                      fast

对于上述数据，最终结果有两个：slow和fast，因此有如下公式：
1. P_slow：代表速度为slow样本所占比例，此处为0.5
2. P_fast：代表速度为fast样本所占比例，此处为0.5
3. -(P_slow\*log2(P_slow) + P_fast\*log2(P_fast)) = -(-0.5-0.5) = 1

表示这个是单一性最差的样本；

## 信息增益
信息增益定义为等于父节点的熵减去子节点的熵的加权平均值，决策树会最大化信息增益来帮助自己在何处进行划分；

## 信息增益计算
对于如下数据：
grade(倾斜)     bumpiness(颠簸)     speed limit(限速)       speed(速度)
steep           bumpy               yes                     slow
steep           smooth              yes                     slow
flat            bumpy               no                      fast
steep           smooth              no                      fast

### 最初根节点
最初的根节点的信息增益为1，前面熵计算那里得到的；

### 根据grade拆分
假设第一个子节点是根据特征**grade**划分，就有以下情况：

a:slow,b:slow,c:fast,d:fast:
1. steep:a,b,d:此时该节点的P_slow为2/3，P_fast为1/3，熵约为0.9183，依然是很差的单一性样本；
2. flat:c:此时该节点的熵为0，因为该节点只有一个样本，满足所有样本属于同一类别；

因此，如果根据grade拆分，信息增益为：

1.0 - (3./4)\*0.9183 - (1./4)\*0 = 0.3113，所谓权重，就是子节点样本数量占父节点的比例；

### 根据bumpiness拆分
现在，尝试根据**bumpiness**拆分：

a:slow,b:slow,c:fast,d:fast:
1. bumpy:a,c:此时该节点的P_slow为1./2，P_fast为1./2，熵为1，最差单一性样本；
2. smooth:b,d:同上；

因此，如果根据bumpiness拆分，信息增益为：

1.0 - (2./4)\*1 - (2./4)\*1 = 0

可以说算法没有从该特征上学到任何信息；

### 根据speed limit拆分
现在，尝试根据**speed limit**拆分：

a:slow,b:slow,c:fast,d:fast:
1. yes:a,b:此时该节点的P_slow为2./2，P_fast为0./2，熵为0；
2. no:c,d:此时该节点的P_slow为0./2，P_fast为2./2，熵为0；

因此，如果根据speed limit拆分，信息增益为： 

1.0 - (2./4)\*0 - (2./4)\*0 = 1 

可以说这个特征是算法能学到最多信息，获取到最大信息增益的特征；

### 信息增益选择
目前来看，选择speed limit最好，直观看也是该特征直接左右了speed；

## 偏差-方差困境
* 高偏差的算法会忽略训练数据，也就是对数据及其不敏感，不管如何填充数据都不会改变算法的行为；
* 高方差的算法则是非常敏感，也导致它只能对训练数据有反应，而无法应对未知的数据，也就是算法无法泛化；

机器学习的一项重要工作就是通过调参来调节偏差和方差，最终取得一个折中的效果，既能通过训练数据来让算法学习，同时又具有足够的泛化能力来应对未知数据；

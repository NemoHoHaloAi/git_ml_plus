# 聚类

## 最基本的聚类算法-k均值
1. 分配：随机给出N个聚类中心点，对于每一个数据点将其归到距离最近的聚类中心点下，形成N个初始聚类；
2. 优化：将每个聚类的中心点向各个方向移动，如果存在使得其到自身所在聚类中所有其他数据点的距离和更小的位置时，将其移动过去；
3. 在第2步优化移动后的基础上，重新进行数据点的分配、优化，直到无法进行优化或者优化后的重新分配对数据点无任何影响为止；

## sklearn-KMeans
重要参数：
1. n_clusters:表示聚类个数，默认为8，但是这个特征太重要了，一般都要根据实际问题来修改为某个值；
2. max_iter:聚类算法的过程就是不停的重复进行分配、优化，该值决定了最多进行多少次迭代；
3. n_init:初始化次数，由于聚类算法非常依赖于初始化给出的聚类中心点的位置，因此可能需要多次不同的初始化才能得到较好的结果；

## KMeans的局限
对于给定的数据集，给定聚类个数，聚类的结果也不总是一致的，因为它还非常依赖于聚类中心初始化的位置，也就是说存在很多初始的情况下，
算法在未找到最好的聚类前就稳定了，这是因为算法本身是一个求局部最优解的问题；

## 单链路聚类-SLC
1. 指定一个K值表示最终需要聚类为几个类；
2. 初始时每个数据点都是一个类；
3. 每个类跟距离自己最近的类进行链接变成一个大类（所谓最近是指这两个类中的某两个点之间的距离最近，例如类A有a,b两个点，类B有c,d两个点，那么如果a距离c很近，类A,B同样可以融合）；
4. 重复3直到剩余K个类为止；

相比KMeans，不受初始化聚类中心点的影响，但是貌似计算复杂度高很多，单链路的单表示计算距离时只考虑当前聚类中的一个点；

## SLC的时间复杂度
n^3

## SLC的局限
由于是单链路的方式，因此可能会忽略了数据点与某个类整体的关系，也就是说存在某个点距离某个类的中心更近，但是距离另一个类的某一个单独的类中的点更近，那么按照
SLC算法，此时是分到后一个类中的，而这似乎不太符合实际情况；

## 软聚类
特点是不强制要求每个点都分到某个聚类中，而是使用概率表示每个点存在与某个类的概率是多少，因此相比其他聚类算法，更加温和；
1. 高斯分布
2. EM-期望最大化

## 聚类的属性
1. 丰富性：得到各种可能的聚类，也就是说不会指定聚类的个数；
2. 比例不变性：切换距离矩阵后，得到的结果不变，也就是说相对位置是不变的；
3. 一致性：改变聚类之间的距离，不会影响聚类的结果；

单链路聚类变种：
1. 聚类到有n/2个类就停止：具备属性2,3，由于指定了聚类个数，因此不满足丰富性；
2. 聚类到不存在两个类之间距离小于x就停止：具备属性1,3，由于指定了具体的聚类间的距离值，因此不满足比例不变性；
3. 剧烈到不存在两个类之间距离小于x/y(y代表所有数据中最远的两个数据点的距离)就停止：具备属性1,2，当仅仅改变某两个本就不在一个距离的点的距离为无限大时，此时的x/y无限小，会导致聚类结果为每个数据点
为一个聚类，这肯定会改变时是不一致的；

不可能存在一种聚类算法满足上述三种属性

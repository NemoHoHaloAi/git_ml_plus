# 交叉验证

## sklearn中使用
cross_validation.train_test_split函数，参数如下：
1. features：全部特征；
2. labels：全部标签；
3. test_size：测试集占比；
4. random_state：随机；

## k折交叉验证
步骤如下：
1. 将原始全部数据平分为k块；
2. 将第一块作为测试集，其余作为训练集进行模型性能评估验证；
3. 重复第2步，直到每一块都分别作为测试集，其余作为训练集验证；
4. 统计这k次循环的结果求平均值；

## sklearn中的k折交叉验证
sklearn.cross_validation.KFold

**注意**：KFold仅仅是将数据进行等量的划分，而没有做任何打乱的操作，如果你的数据中有明显的顺序模式，那么这种划分很可能造成某些块中有特有的数据，而其他的块没有的情况，
进而影响整个模型的性能；

## 交叉验证在模型调参上的应用 -- wahh
GridSearchCV 用于系统地遍历多种参数组合，通过交叉验证确定最佳效果参数。它的好处是，只需增加几行代码，就能遍历多种组合。

下面是来自 sklearn 文档 的一个示例：

    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    svr = svm.SVC()
    clf = grid_search.GridSearchCV(svr, parameters)
    clf.fit(iris.data, iris.target)

逐行说明：

    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}

参数字典以及他们可取的值。在这种情况下，他们在尝试找到 kernel（可能的选择为 'linear' 和 'rbf' ）和 C（可能的选择为1和10）的最佳组合，此种情况在会测试2\*2，也就是4种组合，
各组合均用于训练 SVM，并使用交叉验证对表现进行评估。

    svr = svm.SVC()

这与创建分类器有点类似，就如我们从第一节课一直在做的一样。但是请注意，“clf” 到下一行才会生成—这儿仅仅是在说采用哪种算法。另一种思考方法是，“分类器”在这种情况下不仅仅是一个算法，
而是**算法**加**参数值**。请注意，这里**不需**对 kernel 或 C 做任何尝试，下一行才处理这个问题。

    clf = grid_search.GridSearchCV(svr, parameters)

这是第一个不可思议之处，分类器创建好了。 我们传达算法 (svr) 和参数 (parameters) 字典来尝试，它生成一个网格的参数组合进行尝试。

    clf.fit(iris.data, iris.target)

第二个不可思议之处。 拟合函数现在尝试了所有的参数组合，并返回一个合适的分类器，自动调整至最佳参数组合。现在您便可通过 clf.best_params\_ 来获得参数值。

**PS**：注意，此种方式下，训练时间也会随着组合的增多而成倍增加；

# 支持向量机

## 什么叫SVM
Support Vector Machines，一种使用**超平面**（二维的话就是直线，三维是平面，以此类推）对**两组**数据进行分类的算法，求的是最佳的拟合平面；

## 好的分割线
应该是将margin尽可能大，也就是说分割线与两组数据中最近的点的距离尽可能的远离，这样可以有效增加算法稳定性，避免因噪声引起的分类错误；

## 算法考虑流程
1. 首先考虑正确分类；
2. 其次考虑最大化margin；

## 惩罚值
在实际算法中，不存在所谓的分类正确错误，以及距离远近，不管如何分类数据，每个数据点都对应一个惩罚值，计分如下：
1. 分类错误的给予较大的惩罚值；
2. 分类正确的根据margin大小给予惩罚值，margin越大惩罚值越小，不管margin为何值，对应的惩罚值都比分类错误要小得多；

## SVM特点
因此SVM可以在找到最大margin和忽略异常值之间找到平衡点，对于异常数据稳定性较好；

## 线性分类器
SVM给出的是基于线性分割的分类器，那么它如何给出那些看起来是非线性的决策边界的呢？（其实这些看起来不是线性的边界，在更高维度上可能就是线性的）

## 新特征
当前的特征无法用一个线性分类器分割时，考虑增加新特征，新特征来源于原始的数据，比如二维平面上的点，如果原始的红点都是围绕在离圆心比较近的位置，而蓝点是围绕在远的位置，那么这种
情况就无法是一条直线直接分割，那么就考虑引入新特征，我们看到，似乎红蓝的区别就是离圆心的距离，那么我们就使用增加新特征z=x^2+y^2，那么在x,z坐标系中，这些数据点就又都是线性可分的了，
而此时分割的直线对应到x,y坐标系就是一个距离圆心等距离的一个圆圈；

## 核技巧 -- kernel trick
核函数：接受低维度的输入空间或者说特征空间，映射到高维度空间中，(x,y) -> (x,y,z,....)

所谓核技巧就是通过使用核函数将原本不是线性可分的数据映射到高维度空间进行线性分割，然后再映射回原始空间，得到一个非线性的决策线；

## 机器学习中的参数设置
### 参数C
表示分类的错误项带来的惩罚值，该参数越大，分类器更倾向于尽可能的分类正确而不是追求更大的margin；

## 过拟合
通过调节参数来改善：
1. 参数C；
2. 参数gamma；
3. 参数kernel；
